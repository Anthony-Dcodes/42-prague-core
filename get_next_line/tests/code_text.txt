import matplotlib.pyplot as plt
from sklearn.metrics import (roc_curve, auc, precision_recall_curve, average_precision_score,
                           confusion_matrix, classification_report, precision_score,
                           recall_score, f1_score)
from sklearn.calibration import calibration_curve
import seaborn as sns
import numpy as np

def plot_binary_classification_results_v2(model, X_train, y_train, X_val, y_val, X_test, y_test, threshold=0.5):
    # Get predicted probabilities
    y_train_proba = model.predict_proba(X_train)[:, 1]
    y_val_proba = model.predict_proba(X_val)[:, 1]
    y_test_proba = model.predict_proba(X_test)[:, 1]

    # Get binary predictions using threshold
    y_val_pred = (y_val_proba >= threshold).astype(int)
    y_test_pred = (y_test_proba >= threshold).astype(int)

    # === 1. Plot ROC curves (train / val / test) ===
    fig, axs = plt.subplots(1, 3, figsize=(18, 5))
    for ax, X, y, y_proba, title in zip(
        axs,
        [X_train, X_val, X_test],
        [y_train, y_val, y_test],
        [y_train_proba, y_val_proba, y_test_proba],
        ['Train', 'Validation', 'Test']
    ):
        fpr, tpr, thresholds = roc_curve(y, y_proba)
        roc_auc = auc(fpr, tpr)
        ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
        ax.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate')
        ax.set_ylabel('True Positive Rate')
        ax.set_title(f'ROC Curve - {title}')
        ax.legend(loc='lower right')
    plt.tight_layout()
    plt.show()

    # === 2. Plot histograms of predicted probabilities ===
    fig, axs = plt.subplots(1, 3, figsize=(18, 5))
    for ax, y, y_proba, title in zip(
        axs,
        [y_train, y_val, y_test],
        [y_train_proba, y_val_proba, y_test_proba],
        ['Train', 'Validation', 'Test']
    ):
        sns.histplot(data={'probability': y_proba, 'target': y}, x='probability', hue='target',
                    bins=30, kde=False, palette='Set1', ax=ax, element='step')
        ax.set_title(f'Predicted probabilities - {title}')
        ax.set_xlabel('Predicted probability')
        ax.set_ylabel('Count')
    plt.tight_layout()
    plt.show()

    # === 3. Precision-Recall curve and Calibration curve (same row) ===
    fig, axs = plt.subplots(1, 2, figsize=(16, 6))

    # Precision-Recall curve on validation set
    precision, recall, thresholds = precision_recall_curve(y_val, y_val_proba)
    avg_precision = average_precision_score(y_val, y_val_proba)
    axs[0].plot(recall, precision, color='purple', lw=2, label=f'PR curve (AP = {avg_precision:.4f})')
    axs[0].set_xlabel('Recall')
    axs[0].set_ylabel('Precision')
    axs[0].set_title('Precision-Recall Curve (Validation)')
    axs[0].legend(loc='lower left')
    axs[0].grid(True, alpha=0.3)

    # Calibration curve on validation set
    prob_true, prob_pred = calibration_curve(y_val, y_val_proba, n_bins=10)
    axs[1].plot(prob_pred, prob_true, marker='o', markersize=8, linewidth=2,
               label='Calibration curve', color='red')
    axs[1].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')
    axs[1].set_xlabel('Mean Predicted Probability')
    axs[1].set_ylabel('Fraction of Positives')
    axs[1].set_title('Calibration Curve (Validation)')
    axs[1].legend()
    axs[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # === 4. Confusion Matrix and Precision Metrics Visualization ===
    fig, axs = plt.subplots(1, 2, figsize=(16, 6))

    # Confusion Matrix for Validation Set
    cm_val = confusion_matrix(y_val, y_val_pred)
    sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', ax=axs[0],
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])
    axs[0].set_title('Confusion Matrix (Validation)')
    axs[0].set_ylabel('True Label')
    axs[0].set_xlabel('Predicted Label')

    # Precision, Recall, F1 Score Visualization
    datasets = ['Validation', 'Test']
    y_true_list = [y_val, y_test]
    y_pred_list = [y_val_pred, y_test_pred]

    precision_scores = []
    recall_scores = []
    f1_scores = []

    for y_true, y_pred in zip(y_true_list, y_pred_list):
        precision_scores.append(precision_score(y_true, y_pred))
        recall_scores.append(recall_score(y_true, y_pred))
        f1_scores.append(f1_score(y_true, y_pred))

    x = np.arange(len(datasets))
    width = 0.25

    axs[1].bar(x - width, precision_scores, width, label='Precision', color='skyblue', alpha=0.8)
    axs[1].bar(x, recall_scores, width, label='Recall', color='lightgreen', alpha=0.8)
    axs[1].bar(x + width, f1_scores, width, label='F1-Score', color='salmon', alpha=0.8)

    # Add value labels on bars
    for i, (p, r, f) in enumerate(zip(precision_scores, recall_scores, f1_scores)):
        axs[1].text(i - width, p + 0.01, f'{p:.3f}', ha='center', va='bottom', fontweight='bold')
        axs[1].text(i, r + 0.01, f'{r:.3f}', ha='center', va='bottom', fontweight='bold')
        axs[1].text(i + width, f + 0.01, f'{f:.3f}', ha='center', va='bottom', fontweight='bold')

    axs[1].set_xlabel('Dataset')
    axs[1].set_ylabel('Score')
    axs[1].set_title('Precision, Recall, and F1-Score Comparison')
    axs[1].set_xticks(x)
    axs[1].set_xticklabels(datasets)
    axs[1].legend()
    axs[1].set_ylim(0, 1.1)
    axs[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # === 5. Print Classification Report ===
    print("="*50)
    print("CLASSIFICATION REPORT - VALIDATION SET")
    print("="*50)
    print(classification_report(y_val, y_val_pred))

    print("\n" + "="*50)
    print("CLASSIFICATION REPORT - TEST SET")
    print("="*50)
    print(classification_report(y_test, y_test_pred))

    # === 6. Summary Statistics ===
    print("\n" + "="*50)
    print("SUMMARY STATISTICS")
    print("="*50)
    print(f"Validation Set (threshold = {threshold}):")
    print(f"  Precision: {precision_score(y_val, y_val_pred):.4f}")
    print(f"  Recall:    {recall_score(y_val, y_val_pred):.4f}")
    print(f"  F1-Score:  {f1_score(y_val, y_val_pred):.4f}")
    print(f"  AUC-ROC:   {auc(*roc_curve(y_val, y_val_proba)[:2]):.4f}")
    print(f"  AP Score:  {average_precision_score(y_val, y_val_proba):.4f}")

    print(f"\nTest Set (threshold = {threshold}):")
    print(f"  Precision: {precision_score(y_test, y_test_pred):.4f}")
    print(f"  Recall:    {recall_score(y_test, y_test_pred):.4f}")
    print(f"  F1-Score:  {f1_score(y_test, y_test_pred):.4f}")
    print(f"  AUC-ROC:   {auc(*roc_curve(y_test, y_test_proba)[:2]):.4f}")
    print(f"  AP Score:  {average_precision_score(y_test, y_test_proba):.4f}")
